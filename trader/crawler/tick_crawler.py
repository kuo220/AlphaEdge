import sys
import os
import shutil
import numpy as np
import pandas as pd
import datetime
from loguru import logger
import threading
from concurrent.futures import ThreadPoolExecutor
import time
import random
import requests
from pathlib import Path
from requests.exceptions import ReadTimeout
from requests.exceptions import ConnectionError
import shutil
import shioaji as sj
from typing import List, Optional, Any
import urllib.request
import ipywidgets as widgets
from IPython.display import display
from fake_useragent import UserAgent
from tqdm import tqdm
from tqdm import tnrange, tqdm_notebook
from dateutil.rrule import rrule, DAILY, MONTHLY
from dateutil.relativedelta import relativedelta
sys.path.append(str(Path(__file__).resolve().parents[1]))
from utils import ShioajiAccount, ShioajiAPI
from utils import log_thread
from .crawler_tools import CrawlerTools
from .html_crawler import CrawlHTML
from data import TickDBTools
from config import (LOGS_DIR_PATH, TICK_DOWNLOADS_PATH, TICK_DB_PATH, TICK_DB_NAME, TICK_TABLE_NAME, 
                    API_LIST)


""" 
Shioaji å°è‚¡ ticks è³‡æ–™æ™‚é–“è¡¨ï¼š
From: 2020/03/02 ~ Today

ç›®å‰è³‡æ–™åº«è³‡æ–™æ™‚é–“ï¼š
From 2020/04/01 ~ 2024/05/10
"""

class CrawlStockTick:
    """ çˆ¬å–ä¸Šå¸‚æ«ƒè‚¡ç¥¨ ticks """
    
    def __init__(self):
        """ åˆå§‹åŒ–çˆ¬èŸ²è¨­å®š """
        
        self.api_list: List[sj.Shioaji] = [                                     # Shioaji API List
            api_instance
            for sj_api in API_LIST
            if (api_instance := ShioajiAccount.API_login(sj.Shioaji(), sj_api.api_key, sj_api.api_secret_key)) is not None
        ]
        self.num_threads: int = len(self.api_list)                              # å¯ç”¨çš„ API æ•¸é‡ = å¯é–‹çš„ thread æ•¸
        self.all_stock_list: List[str] = CrawlHTML.crawl_stock_list()           # çˆ¬å–æ‰€æœ‰ä¸Šå¸‚æ«ƒè‚¡ç¥¨æ¸…å–®
        self.split_stock_list: List[List[str]] = []                             # è‚¡ç¥¨æ¸…å–®åˆ†çµ„ï¼ˆå¾ŒçºŒçµ¦å¤šç·šç¨‹ç”¨ï¼‰
        self.table_latest_date: datetime.date = None
        
        # Set logger
        logger.add(f"{LOGS_DIR_PATH}/crawl_stock_tick.log")
        
        # Set downloads directory
        if not os.path.exists(TICK_DOWNLOADS_PATH):
            os.makedirs(TICK_DOWNLOADS_PATH)


    
    def split_list(self, target_list: List[Any], n_parts: int) -> List[List[str]]:
        """ å°‡ list å‡åˆ†æˆ n å€‹ list """
        
        num_list, rem = divmod(len(target_list), n_parts)
        return [target_list[i * num_list + min(i, rem) : (i + 1) * num_list + min(i + 1, rem)] for i in range(n_parts)]


    def crawl_ticks_for_stock(self, api: sj.Shioaji, code: str, date: datetime.date) -> Optional[pd.DataFrame]:
        """ é€é Shioaji çˆ¬å–æŒ‡å®šå€‹è‚¡çš„ tick data """
            
        # åˆ¤æ–· api ç”¨é‡
        if api.usage().remaining_bytes / 1024**2 < 20:
            logger.warning(f"API quota low for {api}. Stopped crawling at stock {code}.")
            return None
        
        try:
            ticks = api.ticks(contract=api.Contracts.Stocks[code], date=date.isoformat())
            tick_df = pd.DataFrame({**ticks})
            
            if not tick_df.empty:
                tick_df.ts = pd.to_datetime(tick_df.ts)
                self.table_latest_date = tick_df.ts.max().date()
            else:
                return None
        except Exception as e:
                logger.error(f"Error Crawling Tick Data: {code} {date} | {e}")
                return None
        
        try:
            formatted_df = TickDBTools.format_tick_data(tick_df, code)
            formatted_df = TickDBTools.format_time_to_microsec(formatted_df)
        
            # Save df to csv file
            formatted_df.to_csv(os.path.join(TICK_DOWNLOADS_PATH, f"{code}.csv"), index=False)
            logger.info(f"Saved {code}.csv to {TICK_DOWNLOADS_PATH}")

        except Exception as e:
            logger.error(f"Error processing or saving tick data for stock {code} | {e}")
        
        return formatted_df
    
    
    @log_thread
    def crawl_ticks_for_stock_list(self, api: sj.Shioaji, stock_list: List[str], start_date: datetime.date, end_date: datetime.date):
        """ é€é Shioaji çˆ¬å–å€‹è‚¡ tick data """
        
        for code in stock_list:
            # åˆ¤æ–· api ç”¨é‡
            if api.usage().remaining_bytes / 1024**2 < 20:
                logger.warning(f"API quota low for {api}. Stopped crawling at stock {code}.")
                break
            
            logger.info(f"Start crawling stock: {code}")
            
            df_list: List[pd.DataFrame] = []   
            cur_date = start_date
            
            while cur_date <= end_date:
                try:
                    ticks = api.ticks(contract=api.Contracts.Stocks[code], date=cur_date.isoformat())
                    tick_df = pd.DataFrame({**ticks})

                    if not tick_df.empty:
                        tick_df.ts = pd.to_datetime(tick_df.ts)
                        self.table_latest_date = tick_df.ts.max().date()
                        df_list.append(tick_df)

                except Exception as e:
                    logger.error(f"Error Crawling Tick Data: {code} {cur_date} | {e}")
                cur_date += datetime.timedelta(days=1)
        
            if not df_list:
                logger.warning(f"No tick data found for stock {code} from {start_date} to {end_date}. Skipping.")
                continue

            # Format tick data
            try:
                merged_df = pd.concat(df_list, ignore_index=True)
                formatted_df = TickDBTools.format_tick_data(merged_df, code)
                formatted_df = TickDBTools.format_time_to_microsec(formatted_df)
            
                # Save df to csv file
                formatted_df.to_csv(os.path.join(TICK_DOWNLOADS_PATH, f"{code}.csv"), index=False)
                logger.info(f"Saved {code}.csv to {TICK_DOWNLOADS_PATH}")
                
            except Exception as e:
                logger.error(f"Error processing or saving tick data for stock {code} | {e}")
            
    
    def crawl_tick_data_multithreaded(self, start_date: datetime.date, end_date: datetime.date):
        """ ä½¿ç”¨ Multi-threading çš„æ–¹å¼ Crawl Tick Data """
        
        logger.info(f"Start multi-thread crawling. Total stocks: {len(self.all_stock_list)}, Threads: {self.num_threads}")
        start_time = time.time()  # ğŸ”¥ é–‹å§‹è¨ˆæ™‚
        
        # å°‡ Stock list å‡åˆ†çµ¦å„å€‹ thread é€²è¡Œçˆ¬èŸ²
        self.split_stock_list = self.split_list(self.all_stock_list, self.num_threads)
        
        # Multi-threading
        with ThreadPoolExecutor(max_workers=self.num_threads) as executor:
            futures = []
            for api, stock_list in zip(self.api_list, self.split_stock_list):
                futures.append(executor.submit(self.crawl_ticks_for_stock_list, api=api, stock_list=stock_list, start_date=start_date, end_date=end_date))

            # ç¢ºä¿åŸ·è¡Œå®Œæ‰€æœ‰çš„ threads æ‰å¾€ä¸‹åŸ·è¡Œå…¶é¤˜ç¨‹å¼ç¢¼
            for future in futures:
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"Thread execution failed with exception: {e}")

        # Update tick table latest date
        TickDBTools.update_tick_table_latest_date(self.table_latest_date)
        
        total_time = time.time() - start_time
        logger.info(f"All crawling tasks completed and metadata updated. Total time: {total_time:.2f} seconds.")
        
    
    def add_to_sql(self):
        """ å°‡è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ CSV æª”å­˜å…¥ tick çš„ DolphinDB ä¸­ """
        pass