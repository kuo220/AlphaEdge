"""
æ¸¬è©¦ StockTickCrawler çš„çˆ¬å–åŠŸèƒ½
åªæ¸¬è©¦çˆ¬å–å’Œæ¸…æ´—ï¼Œä¸å­˜å…¥è³‡æ–™åº«
"""

import datetime
from pathlib import Path
from typing import List, Optional

import pandas as pd
import shioaji as sj
from loguru import logger

from trader.config import TICK_DOWNLOADS_PATH
from trader.pipeline.cleaners.stock_tick_cleaner import StockTickCleaner
from trader.pipeline.crawlers.stock_tick_crawler import StockTickCrawler
from trader.utils import ShioajiAccount, ShioajiAPI, TimeUtils
from trader.config import API_KEY, API_SECRET_KEY


def test_crawler_only(stock_id: str, date: datetime.date):
    """
    åªæ¸¬è©¦çˆ¬å–åŠŸèƒ½ï¼Œä¸ä¿å­˜æª”æ¡ˆ

    Args:
        stock_id: è‚¡ç¥¨ä»£è™Ÿï¼Œä¾‹å¦‚ "2330"
        date: æ—¥æœŸï¼Œä¾‹å¦‚ datetime.date(2024, 1, 15)
    """
    print(f"\n{'='*60}")
    print(f"æ¸¬è©¦çˆ¬å–åŠŸèƒ½ï¼ˆä¸ä¿å­˜æª”æ¡ˆï¼‰")
    print(f"{'='*60}")
    print(f"è‚¡ç¥¨ä»£è™Ÿ: {stock_id}")
    print(f"æ—¥æœŸ: {date}")

    # åˆå§‹åŒ– crawler
    crawler: StockTickCrawler = StockTickCrawler()

    # ç™»å…¥ Shioaji API
    api: sj.Shioaji = sj.Shioaji()
    api_instance: Optional[sj.Shioaji] = ShioajiAccount.API_login(
        api, API_KEY, API_SECRET_KEY
    )

    if api_instance is None:
        print("âŒ API ç™»å…¥å¤±æ•—")
        return None

    print("âœ… API ç™»å…¥æˆåŠŸ")

    # çˆ¬å–è³‡æ–™
    print(f"\né–‹å§‹çˆ¬å– {stock_id} åœ¨ {date} çš„ tick è³‡æ–™...")
    df: Optional[pd.DataFrame] = crawler.crawl_stock_tick(api_instance, date, stock_id)

    if df is None or df.empty:
        print(f"âŒ æ²’æœ‰çˆ¬å–åˆ°è³‡æ–™")
        ShioajiAccount.API_logout(api_instance)
        return None

    print(f"âœ… çˆ¬å–æˆåŠŸï¼")
    print(f"è³‡æ–™ç­†æ•¸: {len(df)}")
    print(f"è³‡æ–™æ¬„ä½: {list(df.columns)}")
    print(f"\nå‰ 5 ç­†è³‡æ–™:")
    print(df.head())

    # ç™»å‡º API
    ShioajiAccount.API_logout(api_instance)

    return df


def test_crawler_and_cleaner(stock_id: str, date: datetime.date):
    """
    æ¸¬è©¦çˆ¬å–å’Œæ¸…æ´—åŠŸèƒ½ï¼Œæœƒä¿å­˜ CSV æª”æ¡ˆåˆ° TICK_DOWNLOADS_PATH

    Args:
        stock_id: è‚¡ç¥¨ä»£è™Ÿï¼Œä¾‹å¦‚ "2330"
        date: æ—¥æœŸï¼Œä¾‹å¦‚ datetime.date(2024, 1, 15)
    """
    print(f"\n{'='*60}")
    print(f"æ¸¬è©¦çˆ¬å–å’Œæ¸…æ´—åŠŸèƒ½ï¼ˆæœƒä¿å­˜ CSV æª”æ¡ˆï¼‰")
    print(f"{'='*60}")
    print(f"è‚¡ç¥¨ä»£è™Ÿ: {stock_id}")
    print(f"æ—¥æœŸ: {date}")
    print(f"è³‡æ–™ä¿å­˜è·¯å¾‘: {TICK_DOWNLOADS_PATH}")

    # åˆå§‹åŒ– crawler å’Œ cleaner
    crawler: StockTickCrawler = StockTickCrawler()
    cleaner: StockTickCleaner = StockTickCleaner()

    # ç™»å…¥ Shioaji API
    api: sj.Shioaji = sj.Shioaji()
    api_instance: Optional[sj.Shioaji] = ShioajiAccount.API_login(
        api, API_KEY, API_SECRET_KEY
    )

    if api_instance is None:
        print("âŒ API ç™»å…¥å¤±æ•—")
        return None

    print("âœ… API ç™»å…¥æˆåŠŸ")

    # çˆ¬å–è³‡æ–™
    print(f"\né–‹å§‹çˆ¬å– {stock_id} åœ¨ {date} çš„ tick è³‡æ–™...")
    df: Optional[pd.DataFrame] = crawler.crawl_stock_tick(api_instance, date, stock_id)

    if df is None or df.empty:
        print(f"âŒ æ²’æœ‰çˆ¬å–åˆ°è³‡æ–™")
        ShioajiAccount.API_logout(api_instance)
        return None

    print(f"âœ… çˆ¬å–æˆåŠŸï¼è³‡æ–™ç­†æ•¸: {len(df)}")

    # æ¸…æ´—è³‡æ–™ï¼ˆæœƒè‡ªå‹•ä¿å­˜ CSVï¼‰
    print(f"\né–‹å§‹æ¸…æ´—è³‡æ–™...")
    cleaned_df: Optional[pd.DataFrame] = cleaner.clean_stock_tick(df, stock_id)

    if cleaned_df is None or cleaned_df.empty:
        print(f"âŒ æ¸…æ´—å¾Œçš„è³‡æ–™ç‚ºç©º")
        ShioajiAccount.API_logout(api_instance)
        return None

    print(f"âœ… æ¸…æ´—æˆåŠŸï¼")
    print(f"æ¸…æ´—å¾Œè³‡æ–™ç­†æ•¸: {len(cleaned_df)}")
    print(f"æ¸…æ´—å¾Œè³‡æ–™æ¬„ä½: {list(cleaned_df.columns)}")
    print(f"\nå‰ 5 ç­†æ¸…æ´—å¾Œçš„è³‡æ–™:")
    print(cleaned_df.head())

    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²ä¿å­˜
    csv_file = TICK_DOWNLOADS_PATH / f"{stock_id}.csv"
    if csv_file.exists():
        file_size = csv_file.stat().st_size
        print(f"\nâœ… CSV æª”æ¡ˆå·²ä¿å­˜: {csv_file}")
        print(f"æª”æ¡ˆå¤§å°: {file_size:,} bytes ({file_size/1024:.2f} KB)")
    else:
        print(f"\nâš ï¸  è­¦å‘Š: CSV æª”æ¡ˆæœªæ‰¾åˆ°æ–¼ {csv_file}")

    # ç™»å‡º API
    ShioajiAccount.API_logout(api_instance)

    return cleaned_df


def test_multiple_dates(stock_id: str, dates: list[datetime.date]):
    """
    æ¸¬è©¦çˆ¬å–å¤šå€‹æ—¥æœŸçš„è³‡æ–™ä¸¦åˆä½µ

    Args:
        stock_id: è‚¡ç¥¨ä»£è™Ÿï¼Œä¾‹å¦‚ "2330"
        dates: æ—¥æœŸåˆ—è¡¨ï¼Œä¾‹å¦‚ [datetime.date(2024, 1, 15), datetime.date(2024, 1, 16)]
    """
    print(f"\n{'='*60}")
    print(f"æ¸¬è©¦çˆ¬å–å¤šå€‹æ—¥æœŸçš„è³‡æ–™")
    print(f"{'='*60}")
    print(f"è‚¡ç¥¨ä»£è™Ÿ: {stock_id}")
    print(f"æ—¥æœŸç¯„åœ: {dates[0]} ~ {dates[-1]} (å…± {len(dates)} å¤©)")
    print(f"è³‡æ–™ä¿å­˜è·¯å¾‘: {TICK_DOWNLOADS_PATH}")

    # åˆå§‹åŒ– crawler å’Œ cleaner
    crawler: StockTickCrawler = StockTickCrawler()
    cleaner: StockTickCleaner = StockTickCleaner()

    # ç™»å…¥ Shioaji API
    api: sj.Shioaji = sj.Shioaji()
    api_instance: Optional[sj.Shioaji] = ShioajiAccount.API_login(
        api, API_KEY, API_SECRET_KEY
    )

    if api_instance is None:
        print("âŒ API ç™»å…¥å¤±æ•—")
        return None

    print("âœ… API ç™»å…¥æˆåŠŸ")

    # çˆ¬å–å¤šå€‹æ—¥æœŸçš„è³‡æ–™
    df_list: List[pd.DataFrame] = []
    for date in dates:
        print(f"\nçˆ¬å– {date} çš„è³‡æ–™...")
        df: Optional[pd.DataFrame] = crawler.crawl_stock_tick(
            api_instance, date, stock_id
        )

        if df is not None and not df.empty:
            df_list.append(df)
            print(f"  âœ… æˆåŠŸï¼Œå–å¾— {len(df)} ç­†è³‡æ–™")
        else:
            print(f"  âš ï¸  æ²’æœ‰è³‡æ–™")

    if not df_list:
        print(f"\nâŒ æ‰€æœ‰æ—¥æœŸéƒ½æ²’æœ‰çˆ¬å–åˆ°è³‡æ–™")
        ShioajiAccount.API_logout(api_instance)
        return None

    # åˆä½µæ‰€æœ‰æ—¥æœŸçš„è³‡æ–™
    merged_df: pd.DataFrame = pd.concat(df_list, ignore_index=True)
    print(f"\nâœ… åˆä½µå®Œæˆï¼ç¸½å…± {len(merged_df)} ç­†è³‡æ–™")

    # æ¸…æ´—è³‡æ–™ï¼ˆæœƒè‡ªå‹•ä¿å­˜ CSVï¼‰
    print(f"\né–‹å§‹æ¸…æ´—è³‡æ–™...")
    cleaned_df: Optional[pd.DataFrame] = cleaner.clean_stock_tick(merged_df, stock_id)

    if cleaned_df is None or cleaned_df.empty:
        print(f"âŒ æ¸…æ´—å¾Œçš„è³‡æ–™ç‚ºç©º")
        ShioajiAccount.API_logout(api_instance)
        return None

    print(f"âœ… æ¸…æ´—æˆåŠŸï¼")
    print(f"æ¸…æ´—å¾Œè³‡æ–™ç­†æ•¸: {len(cleaned_df)}")

    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²ä¿å­˜
    csv_file = TICK_DOWNLOADS_PATH / f"{stock_id}.csv"
    if csv_file.exists():
        file_size = csv_file.stat().st_size
        print(f"\nâœ… CSV æª”æ¡ˆå·²ä¿å­˜: {csv_file}")
        print(f"æª”æ¡ˆå¤§å°: {file_size:,} bytes ({file_size/1024:.2f} KB)")
    else:
        print(f"\nâš ï¸  è­¦å‘Š: CSV æª”æ¡ˆæœªæ‰¾åˆ°æ–¼ {csv_file}")

    # ç™»å‡º API
    ShioajiAccount.API_logout(api_instance)

    return cleaned_df


if __name__ == "__main__":
    # è¨­å®š logger
    logger.remove()  # ç§»é™¤é è¨­çš„ logger
    logger.add(lambda msg: print(msg, end=""), format="{message}")

    # ===== æ¸¬è©¦ç¯„ä¾‹ =====

    # ç¯„ä¾‹ 1: åªæ¸¬è©¦çˆ¬å–åŠŸèƒ½ï¼ˆä¸ä¿å­˜æª”æ¡ˆï¼‰
    print("\n" + "=" * 60)
    print("ç¯„ä¾‹ 1: åªæ¸¬è©¦çˆ¬å–åŠŸèƒ½")
    print("=" * 60)
    test_date = datetime.date(2024, 1, 15)  # è«‹ä¿®æ”¹ç‚ºæ‚¨æƒ³æ¸¬è©¦çš„æ—¥æœŸ
    test_stock = "2330"  # å°ç©é›»ï¼Œè«‹ä¿®æ”¹ç‚ºæ‚¨æƒ³æ¸¬è©¦çš„è‚¡ç¥¨ä»£è™Ÿ
    df1 = test_crawler_only(test_stock, test_date)

    # ç¯„ä¾‹ 2: æ¸¬è©¦çˆ¬å–å’Œæ¸…æ´—åŠŸèƒ½ï¼ˆæœƒä¿å­˜ CSVï¼‰
    print("\n" + "=" * 60)
    print("ç¯„ä¾‹ 2: æ¸¬è©¦çˆ¬å–å’Œæ¸…æ´—åŠŸèƒ½ï¼ˆæœƒä¿å­˜ CSVï¼‰")
    print("=" * 60)
    df2 = test_crawler_and_cleaner(test_stock, test_date)

    # ç¯„ä¾‹ 3: æ¸¬è©¦å¤šå€‹æ—¥æœŸ
    print("\n" + "=" * 60)
    print("ç¯„ä¾‹ 3: æ¸¬è©¦å¤šå€‹æ—¥æœŸ")
    print("=" * 60)
    dates = TimeUtils.generate_date_range(
        datetime.date(2024, 1, 15), datetime.date(2024, 1, 17)
    )
    df3 = test_multiple_dates(test_stock, dates)

    print("\n" + "=" * 60)
    print("æ¸¬è©¦å®Œæˆï¼")
    print("=" * 60)
    print(f"\nğŸ“ è³‡æ–™ä¿å­˜ä½ç½®: {TICK_DOWNLOADS_PATH}")
    print(f"   å¦‚æœåŸ·è¡Œäº†ç¯„ä¾‹ 2 æˆ– 3ï¼ŒCSV æª”æ¡ˆæœƒä¿å­˜åœ¨æ­¤ç›®éŒ„ä¸‹")
    print(f"   æª”æ¡ˆåç¨±æ ¼å¼: {{stock_id}}.csv (ä¾‹å¦‚: 2330.csv)")
